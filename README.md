# Graph Generative Models & Expressiveness
 Mini-project for my CST Part III Representation Learning on Graphs and Networks (L45) module

### Abstract

Graph generation is a very challenging problem that requires predicting an entire graph with multiple nodes and edges from a given label, and is fundamental for many real-world tasks, such as molecular graph generation for drug discovery. Similar to other generative models, a standard graph generative model consists of two parts: a representation module that learns graph embeddings from a pre-training graph space, and a generative module that decodes graph embeddings into a fine-tuning graph space. A lot of successful methods have been explored on graph generation, including GCPN and GraphAF, but the underlying GNN structure for graph representation within both works remains untouched (i.e. R-GCN). In this mini-project, we investigate the expressivity of GNNs under the context of the graph generation problem. We take TorchDrug as a starting point, using the ZINC dataset for pre-training, and improve the state-of-the-art performance on graph generation by replacing R-GCN with more expressive GNNs for graph representation, such as PNA, DGN, GSN and CWN. In addition, since nearly all of the recent works on new GNN architectures are focused on pushing node/graph classification benchmarks, which are comparatively simpler than graph generation modelling in terms of the combinatorial complexity, we also wish to challenge the graph representation learning communityâ€™s notion for benchmarking the expressivity of GNNs with this mini-project.
